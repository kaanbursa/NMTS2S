{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from pipeline import *\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTEncoder(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_size, rnn_hidden_size):\n",
    "        super(NMTEncoder, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_size, padding_idx=0)\n",
    "        self.gru = nn.GRU(embedding_size,rnn_hidden_size, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        \n",
    "    def forward(self,x, x_len):\n",
    "        embedded = self.embedding(x)\n",
    "        # Create packed sequence\n",
    "        x_len = x.len.detach().cpu().numpy()\n",
    "        x_packed = pack_padded_sequence(embedded, x_len, batch_first=True)\n",
    "        \n",
    "        # x_birnn_h.shape = (num_rnn, batch_size, feature_size)\n",
    "        x_birnn_out, x_birnn_h = self.gru(x_packed)\n",
    "        # permute to (batchs, num_rnn, feature_size)\n",
    "        x_birnn_h = x_birnn_h.permute(1, 0, 2)\n",
    "        \n",
    "        #flatten (bz, rnn_hid * feat_size)\n",
    "        x_birnn_h = x_birnn_h.contiguous().view(x_birnn_h.size(0),-1)\n",
    "        \n",
    "        x_unpacked, _ = pad_packed_sequence(x_birnn_out, batch_first = True)\n",
    "        return x_unpacked, x_birnn_h\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "     Args:\n",
    "        num_embeddings (int): number of embeddings; also the number of unique words in the target vocabulary\n",
    "        embedding_size (int): size of the embedding vector \n",
    "        rnn_hidden_size (int): size of the hidden RNN state bos_index(int): BEGIN-OF-SEQUENCE index\n",
    "    \"\"\"\n",
    "    def __init__(self, num_embeddings, embedding_size, rnn_hidden_size, bos_index):\n",
    "        super(NMTDecoder, self).__init__()\n",
    "        self._rnn_hidden_size = rnn_hidden_size\n",
    "        self.t_embedding = nn.Embedding(num_embeddings, embeddings_size, padding_idx=0)\n",
    "        self.gru_cell = nn.GRUCell(embedding_size + rnn_hidden_size,rnn_hidden_size)\n",
    "        self.hidden_map = nn.Linear(rnn_hidden_size, rnn_hidden_size)\n",
    "        self.classifier = nn.Linear(rnn_hidden_size * 2,num_embeddings)\n",
    "        self.bos_index = bos_index\n",
    "        \n",
    "        \n",
    "    def _init_indices(self, batch_size):\n",
    "        return torch.ones(batch_size, dtype=torch.int64) * self.bos_index\n",
    "    \n",
    "    \n",
    "    def _init_context_vectors(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.__rnn_hidden_size)\n",
    "    \n",
    "    def forward(self, encoder_state, init_hidden_state, target_seq):\n",
    "        \"\"\"The forward pass of the model\n",
    "        Args:\n",
    "            encoder_state (torch.Tensor): output of the NMTEncoder \n",
    "            initial_hidden_state (torch.Tensor): last hidden state in the NMTEn \n",
    "            target_sequence (torch.Tensor): target text data tensor \n",
    "            sample_probability (float): schedule sampling parameter\n",
    "            probability of using model's predictions at each decoder step \n",
    "        Returns:\n",
    "            output_vectors (torch.Tensor): prediction vectors at each output st\n",
    "        \"\"\"\n",
    "        \n",
    "        target_seq = target_seq.permute(1,0)\n",
    "        \n",
    "        batch_size = encoder_state.size(0)\n",
    "        \n",
    "        h_t = self.hidden_map(initial_hidden_state)\n",
    "        \n",
    "        context_vectors = self._init_context_vectors(batch_size)\n",
    "        \n",
    "        y_t_indices = self._init_indices(batch_size)\n",
    "        \n",
    "        h_t = h_t.to(encoder_state.device)\n",
    "        y_t_index = y_t_index.to(encoder_state.device)\n",
    "        \n",
    "        context_vectors = context_vectors.to(encoder_state.device)\n",
    "        \n",
    "        output_vectors = []\n",
    "        self._cached_p_attn = []\n",
    "        self._cached_ht = []\n",
    "        self._cached_decoder_state = encoder_state.cpu().detach().numpy()\n",
    "        \n",
    "        output_sequence_size = target_sequence.size(0)\n",
    "        \n",
    "        for i in range(output_sequence_size):\n",
    "            # Step 1: Embed word and concat with previous context\n",
    "            y_input_vector = self.t_embedding(target_sequence[i])\n",
    "            rnn_input = torch.cat([y_input_vector,context_vectors],dim=1)\n",
    "            \n",
    "            # Step 2: Make a GRU step, getting a new hidden vector\n",
    "            h_t = self.gru_cell(rnn_input, h_t)\n",
    "            self._cached_ht.append(h_t)\n",
    "            \n",
    "            # Step 3: Use current hidden vector to attend to atten encoder state\n",
    "            context_vectors, p_attn, _ = verbose_attention(encoder_state_vectors = encoder_state, \n",
    "                                                          query_vector=h_t)\n",
    "            \n",
    "            # auxillary: cache the attention probabilities for vis\n",
    "            self.cached_p_attn.append(p_attn.cpu().detach().numpy())\n",
    "            \n",
    "            # Step 4: use current context vector and hidden state for prediction\n",
    "            prediction_vector = torch.cat((context_vectors, h_t), dim=1)\n",
    "            score_for_y_t_index = self.classifier(prediction_vector)\n",
    "            \n",
    "            output_vectors.append(score_for_y_t_index)\n",
    "            \n",
    "            return score_for_y_t_index\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTModel(nn.Module):\n",
    "    \"\"\" A Neural Translation Model\"\"\"\n",
    "    def __init__(self, source_vocab_size, source_embedding_size, target_vocab_size, target_embedding_size, \n",
    "                encoding_size, target_bos_index):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            source_vocab_size (int): number of unique words in source vocabulary\n",
    "            source_embedding_size (int): size of embedding vector\n",
    "            target_vocab_size(int): number of unique words in target vocabulary\n",
    "            target_embeddiing_size (int): size of target embedding vector\n",
    "            encoding_size (int): size of encoder RNN\n",
    "            target_bos_index (int): index for BEGIN-OF-SEQUENCE token\n",
    "        \"\"\"\n",
    "        \n",
    "        super(NMTModel,self).__init__()\n",
    "        self.encoder = NMTEncoder(num_embeddings=source_vocab_size,\n",
    "                                 embedding_size=source_embedding_size,\n",
    "                                 rnn_hiden_size= encoding_size)\n",
    "        decoding_size = encoding_size * 2\n",
    "        self.decoder = NMTDecoder(num_embeddings=target_vocab_size,\n",
    "                                 embedding_size=target_embedding_size,\n",
    "                                 rnn_hidden_size=decoding_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

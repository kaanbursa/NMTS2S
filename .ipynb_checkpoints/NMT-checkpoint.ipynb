{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceVocabulary():\n",
    "    def __init__(self,token_to_index=None, add_unk=True, unk_token='<UNK>'):\n",
    "        \n",
    "        if token_to_index is None:\n",
    "            token_to_index = {}\n",
    "        self._token_to_index =token_to_index\n",
    "        self._idx_to_token = {k:v for v,k in self._token_to_index.items()}\n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        self.unk_index = -1\n",
    "        \n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "            \n",
    "    \n",
    "        \n",
    "    @classmethod\n",
    "    def create_vocabulary(cls, path):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTVectorizer(object):\n",
    "    \n",
    "    def __init__(self,source_vocab, target_vocab, max_source_length, max_target_length):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            source vocabulary: maps source words to integers\n",
    "            target vocabulart: maps target words to integers\n",
    "            max_source_length: longest sequence in the source dataset\n",
    "            max_target_length: longest sequence in target dataset\n",
    "        \"\"\"\n",
    "        self.source_vocab = source_vocab\n",
    "        self.target_vocab = target_vocab\n",
    "        \n",
    "        self.max_source_length = max_source_length\n",
    "        self.max_target_length = max_target_length\n",
    "        \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, bitext_df):\n",
    "        \"\"\"\n",
    "        Instantiate the vectorizer from the dataset dataframe\n",
    "        \n",
    "        Args:\n",
    "            bitext_df: the parallel text dataset\n",
    "        Returns:\n",
    "            an instance of NMT vectorizer\n",
    "        \"\"\"\n",
    "        source_vocab= SequenceVocabulary()\n",
    "        target_vocab = SequenceVocabulary()\n",
    "        max_source_length, max_target_length = 0,0\n",
    "        \n",
    "        for _, row in bitext_df.iterrows():\n",
    "            source_tokens = row[\"source_language\"].split(\" \")\n",
    "            if len(source_tokens) > max_source_length:\n",
    "                max_source_length = len(source_tokens)\n",
    "            for token in source_tokens:\n",
    "                source_vocab.add_token(token)\n",
    "            \n",
    "            target_tokens = row[\"target_language\"].split(\" \")\n",
    "            if len(target_tokens) > max_target_length:\n",
    "                max_target_length = len(target_tokens)\n",
    "            for token in target_tokens:\n",
    "                target_vocab.add_token(token)\n",
    "                \n",
    "        return cls(source_vocab, target_vocab, max_source_length, max_target_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
